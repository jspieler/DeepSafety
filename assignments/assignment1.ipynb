{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Metrics & Experiment Tracking\n",
    "\n",
    "## Overview \n",
    "In this assigment, you will train your first neural network for traffic sign classification and evaluate its performance using `Batch 1`. \n",
    "Unzip the corresponding batch folder before you proceed. The password is \"Origin\".\n",
    "\n",
    "Complete the tasks below. \n",
    "\n",
    "**Note:** Make sure you hand in all Python files that you changed or created.\n",
    "\n",
    "Learning goals:\n",
    "\n",
    "* Get familiar with the existing code base and demonstrate familiarity with programming skills necessary for this course.\n",
    "* Understand the scope and importance of performance evaluation, metrics and experiment tracking when building a machine learning model.\n",
    "* Discuss different metrics for model evaluation, their scopes and limitations and your decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from pathlib import Path\n",
    "import mlflow\n",
    "import torch.nn as nn\n",
    "\n",
    "from eval import evaluate, load_and_transform_data, get_sign_names\n",
    "\n",
    "ROOT_DIR = Path().cwd().parent\n",
    "BATCH_DIR = ROOT_DIR / \"safetyBatches\" / \"Batch_1\"\n",
    "\n",
    "# Set the tracking server to be localhost with sqlite as tracking store\n",
    "mlflow.set_tracking_uri(uri=f\"sqlite:///{ROOT_DIR / 'mlruns.db'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: make sure you use the MLFlow run ID of your trained model\n",
    "RUN_ID = \"your_run_id\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Track validation results\n",
    "Below, you can find a starting point to validate your model's performance using the `evaluate` method.\n",
    "So far, only the training metrics are tracked using MLFlow. Extend the `evaluate` method to also add evaluation results to the existing MLFlow run. \n",
    "You can find the MLFlow tracking documentation here: https://mlflow.org/docs/latest/tracking.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_uri = f\"runs:/{RUN_ID}/model\"\n",
    "loaded_model = mlflow.pytorch.load_model(model_uri)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "batch_loader = load_and_transform_data(data_directory_path=str(BATCH_DIR))\n",
    "\n",
    "# TODO: track evaluation results\n",
    "predictions = evaluate(loaded_model, criterion, batch_loader)\n",
    "\n",
    "# Output incorrect classifications\n",
    "ground_truth = []\n",
    "for _, target in batch_loader:\n",
    "    ground_truth.extend(target.tolist())\n",
    "sign_names = get_sign_names()\n",
    "wrong_predictions_idx = [\n",
    "    idx for idx, (y_pred, y) in enumerate(zip(predictions, ground_truth)) if y_pred != y\n",
    "]\n",
    "for idx in wrong_predictions_idx:\n",
    "    print(\n",
    "        f\"Traffic sign {sign_names[ground_truth[idx]]} incorrectly classified as {sign_names[predictions[idx]]}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Accuracy is all you need?\n",
    "2.1 At the moment, only the accuracy is tracked as a performance measure.\n",
    "Is accuracy a proper measure to evaluate machine learning algorithms for safety-critical applications?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==> Please answer the question (yes/no) here and briefly explain the reasons for your decision. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2 Think of how you can further increase your validation pipeline by tracking additional measures **and** actually track them (implementation). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==> Please briefly explain the reasons for your decision on which additional metrics to track. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Evaluate the performance of your model\n",
    "Given the results of the two previous tasks, how would you estimate the performance of your model so far?\n",
    "Following questions may serve as an inspiration:\n",
    "\n",
    "- How does your model really perform? \n",
    "- How confident are you about the performance of your model at this point?\n",
    "- Are there any issues with the given batch that you have not taken into consideration yet? If so, how can you tackle them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==> Please answer the question here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepsafety_env3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
